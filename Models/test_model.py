from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import torch

# check the availability of GPU
print("GPU is available:", torch.cuda.is_available())


# define the funtion to load and test model
def load_and_test_model(model_name, model_save_dir, test_sentence):
    """
    load the specified language model and its tokenizer, 
    and test with a sentence to generate output

    Args:
        model_name (str): the name of model
        model_save_dir (str): the path where the model is saved
        test_sentence (str): the sentence used for testing the model
    """

    # print the name of the model which is being loading
    print(f"Loading model {model_name}...")

    # generate the corresponding local save path based on the model name 
    # (replace slashes to avoid file system conflicts).
    model_save_dir = os.path.join(model_save_dir, model_name.replace("/", "_"))

    # load model from local path
    model = AutoModelForCausalLM.from_pretrained(model_save_dir)

    # load tokenizer from local path
    tokenizer = AutoTokenizer.from_pretrained(model_save_dir)

    # print when model is successfully loaded
    print(f"Model {model_name} loaded.")

    # use a tokenizer to encode the input sentence into a tensor for model input.
    inputs = tokenizer(test_sentence, return_tensors="pt")

    # use the model generate output, limiting the maximum number of tokens to 50.
    outputs = model.generate(**inputs, max_new_tokens=50)

    # print the output generated by the model.
    print(f"The output of model {model_name} is: ")
    print(
        tokenizer.decode(outputs[0], skip_special_tokens=True)
    )  

model_save_dir = "/dtu/blackhole/"

# define test sentence
test_sentence = "What is the biological function of insulin?"

# test BioMistral 7B
BioMistral_name = "BioMistral/BioMistral-7B"
load_and_test_model(BioMistral_name, model_save_dir, test_sentence)

# test Mistral 7B Instruct
Instruct_name = "mistralai/Mistral-7B-Instruct-v0.1"
load_and_test_model(Instruct_name, model_save_dir, test_sentence)
